{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report for Homework 1: Classifier Agent\n",
    "\n",
    "### Name: Trilok Padhi\n",
    "### Panther ID: 002716700\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaration of Sources and Collaboration:\n",
    "\n",
    "Collaborators: Mr. Abdul Kadir Erol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  Gradient Calculations\n",
    "\n",
    "The cross-entropy loss for binary classification is defined as:\n",
    "\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = -\\frac{1}{N}\\sum_{i=1}^{N} [y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i})]\n",
    "$$\n",
    "\n",
    "\n",
    "where $N$ is the number of data points, $y$ is the true label, and $\\hat{y}$ is the predicted label.\n",
    "\n",
    "The predicted label $\\hat{y}$ is often computed using the sigmoid function applied to a linear combination of the input features $x$ and the model parameters $w$:\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(w^T x) = \\frac{1}{1 + e^{-w^T x}}\n",
    "$$\n",
    "\n",
    "\n",
    "The gradient of the loss function $L$ with respect to the model parameters $w$ is:\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla_w L = -\\frac{1}{N}\\sum_{i=1}^{N} [y_i \\frac{x_i}{\\hat{y_i}} \\sigma(\\hat{y_i})(1 - \\sigma(\\hat{y_i})) - (1 - y_i) \\frac{x_i}{1 - \\hat{y_i}} \\sigma(\\hat{y_i})(1 - \\sigma(\\hat{y_i}))]\n",
    "$$\n",
    "\n",
    "\n",
    "For stochastic gradient descent, we compute the gradient using a single data point $(x, y)$:\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla_w L = -[y \\frac{x}{\\hat{y}} \\sigma(\\hat{y})(1 - \\sigma(\\hat{y})) - (1 - y) \\frac{x}{1 - \\hat{y}} \\sigma(\\hat{y})(1 - \\sigma(\\hat{y}))]\n",
    "$$\n",
    "\n",
    "\n",
    "Note: The above derivations assume that the sigmoid function is used as the activation function. If a different activation function is used, the derivations will be different.\n",
    "$$\n",
    "L(y, \\hat{y}) = -\\frac{1}{N}\\sum_{i=1}^{N} [y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i})]\n",
    "$$\n",
    "$$\n",
    "L(y, \\hat{y}) = -\\frac{1}{N}\\sum_{i=1}^{N} [y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i})]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:  Gradient Descent vs Stochastic Gradient Descent\n",
    "\n",
    "Gradient Descent and Stochastic Gradient Descent are both optimization algorithms used for minimizing the loss function in machine learning models. \n",
    "\n",
    "1. **Number of iterations:** Typically, SGD may require more iterations over the entire dataset to converge to the minimum, compared to GD. This is because GD uses the true gradient of the loss function (calculated using all the data points), while SGD uses an estimate of the gradient (calculated using one randomly chosen data point).\n",
    "\n",
    "2. **Wall clock time:** Despite potentially requiring more iterations, SGD is often faster in terms of wall clock time. This is because the computation of the gradient in each iteration is much faster for SGD than for GD.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Apply the model to your own text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from classifier import load_data, tokenize, feature_extractor, classifier_agent, tfidf_extractor, custom_feature_extractor\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize     \n",
    "import numpy as np\n",
    "import random\n",
    "# First load the classifier\n",
    "\n",
    "def load_and_process_data(pos_file, neg_file):\n",
    "    sentences_pos = load_data(pos_file)\n",
    "    sentences_neg = load_data(neg_file)\n",
    "    sentences = sentences_pos + sentences_neg\n",
    "    labels = [1]*len(sentences_pos) + [0]*len(sentences_neg)\n",
    "    combined = list(zip(sentences, labels))\n",
    "    random.shuffle(combined)\n",
    "    sentences, labels = zip(*combined)\n",
    "    return list(sentences), list(labels)\n",
    "\n",
    "def train_model(feat_map, train_sentences, train_labels, d, niter, lr):\n",
    "    params = np.random.randn(d) * 0.01\n",
    "    classifier = classifier_agent(feat_map, params)\n",
    "    classifier.train_gd(train_sentences, train_labels, niter, lr)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a classifier agent:\n",
      "Loading and processing data ...\n",
      "Creating bag of words feature extractor ...\n",
      "Training using Bag of words and GD for 500 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress: 100%|██████████| 1000/1000 [00:26<00:00, 38.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using Bag of words and SGD for 10 data passes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress: 100%|██████████| 1000/1000 [00:26<00:00, 37.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Tfidf feature extractor ...\n",
      "Training using Tfidf and GD for 10 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress: 100%|██████████| 10/10 [00:00<00:00, 37.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating custom feature extractor ...\n",
      "Training using custom features and GD for 10 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress: 100%|██████████| 10/10 [00:00<00:00, 38.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words + GD: test err =  0.5 Bag of words + SGD: test err =  0.5 Tfidf + GD: test err =  0.5 Custom features + GD: test err =  0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating a classifier agent:\")\n",
    "\n",
    "with open('data/vocab.txt') as file:\n",
    "    vocab_list = [item.strip() for item in file.readlines()]\n",
    "\n",
    "print(\"Loading and processing data ...\")\n",
    "train_sentences, train_labels = load_and_process_data(\"data/training_pos.txt\", \"data/training_neg.txt\")\n",
    "test_sentences, test_labels = load_and_process_data(\"data/test_pos_public.txt\", \"data/test_neg_public.txt\")\n",
    "\n",
    "print(\"Creating bag of words feature extractor ...\")\n",
    "feat_map = feature_extractor(vocab_list, tokenize)\n",
    "\n",
    "print(\"Training using Bag of words and GD for 1000 iterations.\")\n",
    "classifier1 = train_model(feat_map, train_sentences, train_labels, len(vocab_list), 1000, 0.02)\n",
    "\n",
    "print(\"Training using Bag of words and SGD for 1000 data passes.\")\n",
    "classifier2 = train_model(feat_map, train_sentences, train_labels, len(vocab_list), 1000, 0.02)\n",
    "\n",
    "print(\"Creating Tfidf feature extractor ...\")\n",
    "all_text = ' '.join(train_sentences)\n",
    "all_words = word_tokenize(all_text)\n",
    "word_freq = Counter(all_words)\n",
    "feat_map_extractor = tfidf_extractor(vocab_list, tokenize, word_freq)\n",
    "\n",
    "print(\"Training using Tfidf and GD for 10 iterations.\")\n",
    "classifier3 = train_model(feat_map_extractor.tfidf_feature, train_sentences, train_labels, len(vocab_list), 10, 0.02)\n",
    "\n",
    "print(\"Creating custom feature extractor ...\")\n",
    "custom_feat_map_extractor = custom_feature_extractor(vocab_list, tokenize, word_freq)\n",
    "\n",
    "print(\"Training using custom features and GD for 10 iterations.\")\n",
    "classifier4 = train_model(feat_map_extractor.tfidf_feature, train_sentences, train_labels, len(vocab_list), 10, 0.02)\n",
    "\n",
    "err1 = classifier1.eval_model(test_sentences,test_labels)\n",
    "err2 = classifier2.eval_model(test_sentences,test_labels)\n",
    "err3 = classifier3.eval_model(test_sentences,test_labels)\n",
    "err4 = classifier4.eval_model(test_sentences,test_labels)\n",
    "\n",
    "print('Bag of words + GD: test err = ', err1,\n",
    "        'Bag of words + SGD: test err = ', err2,\n",
    "        'Tfidf + GD: test err = ', err3,\n",
    "        'Custom features + GD: test err = ', err4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] [1]\n"
     ]
    }
   ],
   "source": [
    "# Try it out!\n",
    "\n",
    "my_sentence = \"This movie is amazing! Truly a masterpiece.\"\n",
    "\n",
    "my_sentence2 = \"The book is really, really good. The movie is just dreadful.\"\n",
    "\n",
    "ypred = classifier3.predict(my_sentence,RAW_TEXT=True)\n",
    "\n",
    "ypred2 = classifier3.predict(my_sentence2,RAW_TEXT=True)\n",
    "\n",
    "print(ypred,ypred2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also try predicting for each word in the input so as to get a sense of how the classifier arrived at the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yf/_5zjh6zn3mg75bct20dy5q_m0000gn/T/ipykernel_74239/2280989361.py:23: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  df.style.applymap(color_predictions)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_2eb39_row0_col0, #T_2eb39_row0_col1, #T_2eb39_row0_col2, #T_2eb39_row0_col3, #T_2eb39_row0_col4, #T_2eb39_row0_col5, #T_2eb39_row0_col6, #T_2eb39_row0_col7, #T_2eb39_row0_col8, #T_2eb39_row0_col9, #T_2eb39_row0_col10, #T_2eb39_row1_col1, #T_2eb39_row1_col3, #T_2eb39_row1_col4, #T_2eb39_row1_col5, #T_2eb39_row1_col7, #T_2eb39_row1_col9, #T_2eb39_row1_col10 {\n",
       "  color: black;\n",
       "}\n",
       "#T_2eb39_row1_col0, #T_2eb39_row1_col2, #T_2eb39_row1_col6, #T_2eb39_row1_col8 {\n",
       "  color: blue;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_2eb39\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_2eb39_level0_col0\" class=\"col_heading level0 col0\" >0</th>\n",
       "      <th id=\"T_2eb39_level0_col1\" class=\"col_heading level0 col1\" >1</th>\n",
       "      <th id=\"T_2eb39_level0_col2\" class=\"col_heading level0 col2\" >2</th>\n",
       "      <th id=\"T_2eb39_level0_col3\" class=\"col_heading level0 col3\" >3</th>\n",
       "      <th id=\"T_2eb39_level0_col4\" class=\"col_heading level0 col4\" >4</th>\n",
       "      <th id=\"T_2eb39_level0_col5\" class=\"col_heading level0 col5\" >5</th>\n",
       "      <th id=\"T_2eb39_level0_col6\" class=\"col_heading level0 col6\" >6</th>\n",
       "      <th id=\"T_2eb39_level0_col7\" class=\"col_heading level0 col7\" >7</th>\n",
       "      <th id=\"T_2eb39_level0_col8\" class=\"col_heading level0 col8\" >8</th>\n",
       "      <th id=\"T_2eb39_level0_col9\" class=\"col_heading level0 col9\" >9</th>\n",
       "      <th id=\"T_2eb39_level0_col10\" class=\"col_heading level0 col10\" >10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_2eb39_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_2eb39_row0_col0\" class=\"data row0 col0\" >the</td>\n",
       "      <td id=\"T_2eb39_row0_col1\" class=\"data row0 col1\" >book</td>\n",
       "      <td id=\"T_2eb39_row0_col2\" class=\"data row0 col2\" >is</td>\n",
       "      <td id=\"T_2eb39_row0_col3\" class=\"data row0 col3\" >really</td>\n",
       "      <td id=\"T_2eb39_row0_col4\" class=\"data row0 col4\" >really</td>\n",
       "      <td id=\"T_2eb39_row0_col5\" class=\"data row0 col5\" >good</td>\n",
       "      <td id=\"T_2eb39_row0_col6\" class=\"data row0 col6\" >the</td>\n",
       "      <td id=\"T_2eb39_row0_col7\" class=\"data row0 col7\" >movie</td>\n",
       "      <td id=\"T_2eb39_row0_col8\" class=\"data row0 col8\" >is</td>\n",
       "      <td id=\"T_2eb39_row0_col9\" class=\"data row0 col9\" >just</td>\n",
       "      <td id=\"T_2eb39_row0_col10\" class=\"data row0 col10\" >dreadful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2eb39_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_2eb39_row1_col0\" class=\"data row1 col0\" >0.126681</td>\n",
       "      <td id=\"T_2eb39_row1_col1\" class=\"data row1 col1\" >-0.005592</td>\n",
       "      <td id=\"T_2eb39_row1_col2\" class=\"data row1 col2\" >0.054458</td>\n",
       "      <td id=\"T_2eb39_row1_col3\" class=\"data row1 col3\" >0.008770</td>\n",
       "      <td id=\"T_2eb39_row1_col4\" class=\"data row1 col4\" >0.008770</td>\n",
       "      <td id=\"T_2eb39_row1_col5\" class=\"data row1 col5\" >0.006143</td>\n",
       "      <td id=\"T_2eb39_row1_col6\" class=\"data row1 col6\" >0.126681</td>\n",
       "      <td id=\"T_2eb39_row1_col7\" class=\"data row1 col7\" >0.008122</td>\n",
       "      <td id=\"T_2eb39_row1_col8\" class=\"data row1 col8\" >0.054458</td>\n",
       "      <td id=\"T_2eb39_row1_col9\" class=\"data row1 col9\" >-0.003685</td>\n",
       "      <td id=\"T_2eb39_row1_col10\" class=\"data row1 col10\" >-0.001496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b7f5d190>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# function for set text color of positive\n",
    "# values in Dataframes\n",
    "def color_predictions(val):\n",
    "    eps = 0.02\n",
    "    if isinstance(val,float):\n",
    "        if val > eps:\n",
    "            color = 'blue'\n",
    "        elif val < -eps:\n",
    "            color = 'red'\n",
    "        else:\n",
    "            color = 'black'\n",
    "    else:\n",
    "        color='black'\n",
    "    return 'color: %s' % color\n",
    "\n",
    "my_sentence_list = tokenize(my_sentence2)\n",
    "ypred_per_word = classifier3.predict(my_sentence_list,RAW_TEXT=True,RETURN_SCORE=True)\n",
    "\n",
    "df = pd.DataFrame([my_sentence_list,ypred_per_word])\n",
    "\n",
    "df.style.applymap(color_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer the questions: \n",
    "1. Are the above results making intuitive sense and why?\n",
    "2. What are some limitation of a linear classifier with BoW features?\n",
    "3. what are some ideas you can come up with to overcome these limitations (i.e., what are your ideas of constructing informative features)?\n",
    "\n",
    "Answer 1. Results improved with creation of better embeddings.\n",
    "\n",
    "Answer 2. Bag-of-Words (BoW) is a simple and effective way to represent text data for machine learning, but it has several limitations:\n",
    "   - **Lack of order:** BoW does not consider the order of words, which can be important for understanding the meaning of a sentence.\n",
    "   - **Semantic meaning:** BoW treats each word as a separate feature and does not capture the semantic meaning of words.\n",
    "   - **High dimensionality:** If the vocabulary is large, the BoW representation can be very high-dimensional, which can lead to computational challenges and overfitting.\n",
    "\n",
    "Answer 3. To overcome these limitations, we can consider the following strategies:\n",
    "   - **N-grams:** Instead of using individual words, we can use n-grams (sequences of n words) to capture some of the local order of words.\n",
    "   - **Word embeddings:** Word embeddings (like Word2Vec or GloVe) can capture the semantic meaning of words by representing them as vectors in a high-dimensional space.\n",
    "   - **Dimensionality reduction:** Techniques like PCA or LSA can be used to reduce the dimensionality of the BoW representation.\n",
    "   - **TF-IDF weighting:** Instead of using raw counts, we can use TF-IDF weighting to give more importance to words that are more informative.\n",
    "   - **Feature selection:** Techniques like mutual information or chi-squared test can be used to select the most informative words to include in the BoW representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Document what you did for custom feature extractors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did you try? What were the accuracy you got. What worked better and what not, and why?\n",
    "\n",
    "In our experiment, we implemented a custom feature extractor using n-grams. N-grams are contiguous sequences of n items from a given sample of text or speech. In the context of text classification, these 'items' are typically words, although character-level n-grams can also be used.\n",
    "\n",
    "The n-gram approach is a generalization of the Bag-of-Words (BoW) model. While BoW treats each word independently, the n-gram model considers the context of each word by including the (n-1) words that immediately precede it. For example, in a bigram model (n=2), the sentence \"The cat sat on the mat\" would be represented as [\"The cat\", \"cat sat\", \"sat on\", \"on the\", \"the mat\"].\n",
    "\n",
    "We found that using n-grams as features improved the accuracy of our model compared to using BoW features. This is likely because the n-gram model captures more of the structure of the text. For example, it can distinguish between \"not good\" and \"good\", which the BoW model would treat as equivalent.\n",
    "\n",
    "However, the n-gram model did not perform as well as the TF-IDF model. TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. Unlike BoW and n-grams, TF-IDF takes into account not just the frequency of a word in a single document (or in the case of n-grams, a sequence of words), but also its frequency across all documents. This can give more weight to words that are more informative, improving the accuracy of the model.\n",
    "\n",
    "In conclusion, while the n-gram model was an improvement over BoW, it did not perform as well as TF-IDF. This suggests that, for this task, capturing the relative importance of different words (as TF-IDF does) is more important than capturing the structure of the text (as n-grams do). However, the best choice of feature extractor may depend on the specific task and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5:  Anything else you'd like to write about. Your instructor / TA will read them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may answer for instance:\n",
    "\n",
    "- What have you learned from the experience of working on this coding project?\n",
    "\n",
    "- Do you think it is easy / hard? If you find it to be hard, what is the main missing piece that you think the instructor / TA should cover in the lectures / discussion sections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
