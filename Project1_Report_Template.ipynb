{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report for Homework 1: Classifier Agent\n",
    "\n",
    "### Name:\n",
    "### Panther ID:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaration of Sources and Collaboration:\n",
    "\n",
    "You should declare who you worked with in this homework. Don't submit identical works, rather study together and apply it individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  Gradient Calculations\n",
    "\n",
    "The loss function to use is the cross-entropy loss, averaged over data points.\n",
    "\n",
    "You should include your work for deriving the full gradient and stochastic gradient here.\n",
    "\n",
    "You can type equations using LATEX in markdown with, e.g. $\n",
    "e = mc^2, v = \\frac{dx}{dt}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:  Gradient Descent vs Stochastic Gradient Descent\n",
    "\n",
    "Discuss what do you observe about GD and SGD from your implementation? Which one is faster in terms of number of iterations, which one is faster in terms of the wall clock time?\n",
    "\n",
    "You can plot the learning curves, e.g., training error against epochs and wall-clock time.\n",
    "\n",
    "You can use matplotlib for plotting such figures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Apply the model to your own text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier import load_data,tokenize, compute_word_idf\n",
    "from classifier import custom_feature_extractor, classifier_agent\n",
    "import numpy as np\n",
    "\n",
    "# First load the classifier\n",
    "\n",
    "with open('data/vocab.txt') as file:\n",
    "    reading = file.readlines()\n",
    "    vocab_list = [item.strip() for item in reading]\n",
    "\n",
    "feat_map = custom_feature_extractor(vocab_list, tokenize)\n",
    "\n",
    "d = len(vocab_list)\n",
    "params = np.array([0.0 for i in range(d)])\n",
    "custom_classifier = classifier_agent(feat_map, params)\n",
    "# custom_classifier.load_params_from_file('best_model.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True] [False]\n"
     ]
    }
   ],
   "source": [
    "# Try it out!\n",
    "\n",
    "my_sentence = \"This movie is amazing! Truly a masterpiece.\"\n",
    "\n",
    "my_sentence2 = \"The book is really, really good. The movie is just dreadful.\"\n",
    "\n",
    "ypred = custom_classifier.predict(my_sentence,RAW_TEXT=True)\n",
    "\n",
    "ypred2 = custom_classifier.predict(my_sentence2,RAW_TEXT=True)\n",
    "\n",
    "print(ypred,ypred2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also try predicting for each word in the input so as to get a sense of how the classifier arrived at the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_192f51f8_b448_11ec_881c_f2189867c1cbrow0_col0,#T_192f51f8_b448_11ec_881c_f2189867c1cbrow0_col1,#T_192f51f8_b448_11ec_881c_f2189867c1cbrow0_col2,#T_192f51f8_b448_11ec_881c_f2189867c1cbrow0_col3,#T_192f51f8_b448_11ec_881c_f2189867c1cbrow0_col4,#T_192f51f8_b448_11ec_881c_f2189867c1cbrow0_col5,#T_192f51f8_b448_11ec_881c_f2189867c1cbrow0_col6,#T_192f51f8_b448_11ec_881c_f2189867c1cbrow0_col7,#T_192f51f8_b448_11ec_881c_f2189867c1cbrow0_col8,#T_192f51f8_b448_11ec_881c_f2189867c1cbrow0_col9,#T_192f51f8_b448_11ec_881c_f2189867c1cbrow0_col10,#T_192f51f8_b448_11ec_881c_f2189867c1cbrow1_col0,#T_192f51f8_b448_11ec_881c_f2189867c1cbrow1_col6{\n",
       "            color:  black;\n",
       "        }#T_192f51f8_b448_11ec_881c_f2189867c1cbrow1_col1,#T_192f51f8_b448_11ec_881c_f2189867c1cbrow1_col9,#T_192f51f8_b448_11ec_881c_f2189867c1cbrow1_col10{\n",
       "            color:  red;\n",
       "        }#T_192f51f8_b448_11ec_881c_f2189867c1cbrow1_col2,#T_192f51f8_b448_11ec_881c_f2189867c1cbrow1_col3,#T_192f51f8_b448_11ec_881c_f2189867c1cbrow1_col4,#T_192f51f8_b448_11ec_881c_f2189867c1cbrow1_col5,#T_192f51f8_b448_11ec_881c_f2189867c1cbrow1_col7,#T_192f51f8_b448_11ec_881c_f2189867c1cbrow1_col8{\n",
       "            color:  blue;\n",
       "        }</style><table id=\"T_192f51f8_b448_11ec_881c_f2189867c1cb\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0</th>        <th class=\"col_heading level0 col1\" >1</th>        <th class=\"col_heading level0 col2\" >2</th>        <th class=\"col_heading level0 col3\" >3</th>        <th class=\"col_heading level0 col4\" >4</th>        <th class=\"col_heading level0 col5\" >5</th>        <th class=\"col_heading level0 col6\" >6</th>        <th class=\"col_heading level0 col7\" >7</th>        <th class=\"col_heading level0 col8\" >8</th>        <th class=\"col_heading level0 col9\" >9</th>        <th class=\"col_heading level0 col10\" >10</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_192f51f8_b448_11ec_881c_f2189867c1cblevel0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_192f51f8_b448_11ec_881c_f2189867c1cbrow0_col0\" class=\"data row0 col0\" >the</td>\n",
       "                        <td id=\"T_192f51f8_b448_11ec_881c_f2189867c1cbrow0_col1\" class=\"data row0 col1\" >book</td>\n",
       "                        <td id=\"T_192f51f8_b448_11ec_881c_f2189867c1cbrow0_col2\" class=\"data row0 col2\" >is</td>\n",
       "                        <td id=\"T_192f51f8_b448_11ec_881c_f2189867c1cbrow0_col3\" class=\"data row0 col3\" >really</td>\n",
       "                        <td id=\"T_192f51f8_b448_11ec_881c_f2189867c1cbrow0_col4\" class=\"data row0 col4\" >really</td>\n",
       "                        <td id=\"T_192f51f8_b448_11ec_881c_f2189867c1cbrow0_col5\" class=\"data row0 col5\" >good</td>\n",
       "                        <td id=\"T_192f51f8_b448_11ec_881c_f2189867c1cbrow0_col6\" class=\"data row0 col6\" >the</td>\n",
       "                        <td id=\"T_192f51f8_b448_11ec_881c_f2189867c1cbrow0_col7\" class=\"data row0 col7\" >movie</td>\n",
       "                        <td id=\"T_192f51f8_b448_11ec_881c_f2189867c1cbrow0_col8\" class=\"data row0 col8\" >is</td>\n",
       "                        <td id=\"T_192f51f8_b448_11ec_881c_f2189867c1cbrow0_col9\" class=\"data row0 col9\" >just</td>\n",
       "                        <td id=\"T_192f51f8_b448_11ec_881c_f2189867c1cbrow0_col10\" class=\"data row0 col10\" >dreadful</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_192f51f8_b448_11ec_881c_f2189867c1cblevel0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_192f51f8_b448_11ec_881c_f2189867c1cbrow1_col0\" class=\"data row1 col0\" >-0.017113</td>\n",
       "                        <td id=\"T_192f51f8_b448_11ec_881c_f2189867c1cbrow1_col1\" class=\"data row1 col1\" >-0.197179</td>\n",
       "                        <td id=\"T_192f51f8_b448_11ec_881c_f2189867c1cbrow1_col2\" class=\"data row1 col2\" >0.025361</td>\n",
       "                        <td id=\"T_192f51f8_b448_11ec_881c_f2189867c1cbrow1_col3\" class=\"data row1 col3\" >0.032491</td>\n",
       "                        <td id=\"T_192f51f8_b448_11ec_881c_f2189867c1cbrow1_col4\" class=\"data row1 col4\" >0.032491</td>\n",
       "                        <td id=\"T_192f51f8_b448_11ec_881c_f2189867c1cbrow1_col5\" class=\"data row1 col5\" >0.110763</td>\n",
       "                        <td id=\"T_192f51f8_b448_11ec_881c_f2189867c1cbrow1_col6\" class=\"data row1 col6\" >-0.017113</td>\n",
       "                        <td id=\"T_192f51f8_b448_11ec_881c_f2189867c1cbrow1_col7\" class=\"data row1 col7\" >0.047433</td>\n",
       "                        <td id=\"T_192f51f8_b448_11ec_881c_f2189867c1cbrow1_col8\" class=\"data row1 col8\" >0.025361</td>\n",
       "                        <td id=\"T_192f51f8_b448_11ec_881c_f2189867c1cbrow1_col9\" class=\"data row1 col9\" >-0.094084</td>\n",
       "                        <td id=\"T_192f51f8_b448_11ec_881c_f2189867c1cbrow1_col10\" class=\"data row1 col10\" >-0.132908</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x110943190>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# function for set text color of positive\n",
    "# values in Dataframes\n",
    "def color_predictions(val):\n",
    "    eps = 0.02\n",
    "    if isinstance(val,float):\n",
    "        if val > eps:\n",
    "            color = 'blue'\n",
    "        elif val < -eps:\n",
    "            color = 'red'\n",
    "        else:\n",
    "            color = 'black'\n",
    "    else:\n",
    "        color='black'\n",
    "    return 'color: %s' % color\n",
    "\n",
    "my_sentence_list = tokenize(my_sentence2)\n",
    "ypred_per_word = custom_classifier.predict(my_sentence_list,RAW_TEXT=True,RETURN_SCORE=True)\n",
    "\n",
    "df = pd.DataFrame([my_sentence_list,ypred_per_word])\n",
    "\n",
    "df.style.applymap(color_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer the questions: \n",
    "1. Are the above results making intuitive sense and why?\n",
    "2. What are some limitation of a linear classifier with BoW features?\n",
    "3. what are some ideas you can come up with to overcome these limitations (i.e., what are your ideas of constructing informative features)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Document what you did for custom feature extractors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did you try? What were the accuracy you got. What worked better and what not, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5:  Anything else you'd like to write about. Your instructor / TA will read them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may answer for instance:\n",
    "\n",
    "- What have you learned from the experience of working on this coding project?\n",
    "\n",
    "- Do you think it is easy / hard? If you find it to be hard, what is the main missing piece that you think the instructor / TA should cover in the lectures / discussion sections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
